# [NETicket] 대규모 트랜잭션을 빠르고 안정적으로 처리하는 티켓 예매 사이트 (2조)

![NETicket-PPT_복사본-001 (1).png](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/NETicket-PPT_%25EB%25B3%25B5%25EC%2582%25AC%25EB%25B3%25B8-001_(1).png)

## 📚 ***목차***

# 💁🏻‍♂️01 | 프로젝트 소개

```jsx
🎫 **NETicket** 
    
**No Error Ticket!**
동시접속자가 많아져도 **No Error!**
남녀노소 누구나 내 티켓은 NETicket에서 쉽고 편리하게! 
   
저희 서비스는 **간편**하고 **직관**적인 레이아웃으로,
불편한 예매 과정을 최소화해 모두가 쉽게 이용할 수 있는 예매 사이트입니다.

다양한 화면 크기와 장치에서 일관된 사용자 경험을 제공할 수 있도록,
**반응형 디자인**으로 설계했습니다. 

대규모 트래픽 부하 테스트를 하며, 
**최소 비용**으로 시스템의 안정성과 성능 향상에 중점을 두고 프로젝트를 진행했습니다. 

한 번에 **50K**의 예매 요청이 발생했을 때,
**1000 TPS**의 처리량으로 평균 응답속도 약 1초 내외로 Error 없이 운영 중입니다. 
```

---

# 💻02 | 주요기능

### 📌 Redis Cache를 사용한 빠른 예매

![예매2.gif](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/%25EC%2598%2588%25EB%25A7%25A42.gif)

- Scheduling 기능을 이용하여 예매 오픈 시간에 공연의 남은 좌석 수를 Redis Cache에 데이터를 자동으로 업데이트하는 기능을 구현
- 고객이 선택한 티켓 수만큼 Write Back 방식으로 Redis Cache에 있는 공연의 남은 좌석 수를 차감해 빠른 응답 시간 도출
- 한 번에 50K 이상의 요청을 평균 응답시간 약 1초 내외로 예매 가능

### 📌 Admin 공연관리 페이지

![관리자.gif](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/%25EA%25B4%2580%25EB%25A6%25AC%25EC%259E%2590.gif)

- 관리자만 접근 가능한 Admin 공연 관리 페이지에서 공연 추가를 할 수 있으며, 추가되는 공연의 이미지 업로드는 AWS S3를 통해 안정적으로 처리
- Redis Cache에 자동 갱신 기능을 추가해, 데이터가 자동으로 업데이트되지만, 장애 상황을 대비해 관리자 제어 시스템 추가. 이를 통해 데이터의 무결성과 가용성을 보장 가능
- Redis Cache에 이상이 생기면 Redis CLI를 통하지 않고, 관리자 페이지에서 쉽고 직관적으로 관리할 수 있도록 구성

### 📌 QueryDSL을 사용한 검색

![검색2.gif](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/%25EA%25B2%2580%25EC%2583%25892.gif)

- 키워드 공백 기준으로 분할해 여러 단어로 구성된 키워드 처리
- title과 place에서 대소문자 구분 없이 단어를 포함하는 event를 찾는 검색 조건
- 예매 가능 여부와, 날짜를 고려한 정렬 순서
- 검색 조건에 일치하는 전체 이벤트 수 계산을 이용한 Pagination

### 📌 마이페이지 예매취소

![마이페이지.gif](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/%25EB%25A7%2588%25EC%259D%25B4%25ED%258E%2598%25EC%259D%25B4%25EC%25A7%2580.gif)

- 마이페이지에서 사용자가 가장 최근에 예매한 공연부터 정렬
- 아직 시작하지 않은 공연만 사용자가 쉽고 빠르게 예매 취소 가능

---

# 🎀03 | 프로젝트 챌린지 포인트

## 📈 대규모 트랜잭션 상황에서의 안정적인 동시성 제어

- 📈 사용자의 원활하고 끊김 없는 서비스 제공 위한 높은 TPS와 빠른 응답속도
    
    ```java
    저희 프로젝트의 주요 목표는 사용자가 항상 원활하고 끊김 없는 서비스를 이용할 수 있도록 하는 것입니다. 이를 위해, 대규모 트랜잭션 상황에서도 안정적이면서도 높은 TPS와 빠른 응답속도를 제공하기 위해 다양한 기술적 요소를 적절하게 활용하였습니다.
    
    우선, 분산 처리 아키텍처와 In-memory caching, Database Tuning 등의 기술을 조합하여 안정적이면서도 높은 TPS와 빠른 응답속도를 실현하였습니다. 
    특히, Redis Cache를 활용하여 In-memory Data Store를 구축하여 응답속도를 크게 향상했습니다.
    
    또한, 부하 분산을 위한 Load Balancing과 자원 확장 및 축소를 자동으로 처리하는 Auto Scaling을 도입하여, 서버 부하를 적절하게 분산하고 트래픽 변화에 따라 적절한 자원을 할당함으로써, 트래픽 급증 시에도 끊김 없는 서비스를 제공할 수 있도록 구성하였습니다. 
    
    마지막으로, HikariCP와 MySQL을 Tuning 하여 Database 연결을 최적화하고, 서버 분산을 통해 병목 현상을 예방하여 안정적인 서비스를 구현하였습니다. 
    
    이러한 다양한 기술적 요소들을 적절하게 조합하여, 저희 서비스는 높은 성능과 안정성을 동시에 유지할 수 있게 되었습니다. 이를 통해 사용자들은 언제나 원활하고 끊김 없는 서비스를 경험할 수 있게 되었습니다.
    ```
    
- 🕸 데이터의 정확성과 일관성을 보장해, 데이터 무결성 확보
    
    ```java
    저희는 Redis를 도입하여 높은 TPS와 빠른 응답 속도를 확보하였으나, 중복 데이터로 인한 데이터의 일관성과 정확성 문제가 생겼습니다. 이에 대응하여 데이터 무결성을 확보하기 위해 아래와 같은 캐시 전략을 수립하였습니다.
    
    먼저 쓰기 전략으로 Write Back 방식을 도입하여, 티켓의 남은 좌석 수 데이터 수정 시 캐시에만 변경사항이 기록되고, 주기적으로 또는 특정 조건이 충족될 때 Database에 동기화합니다. 이를 통해 빠른 응답 시간과 Database의 부하를 줄일 수 있습니다. 
    
    특히, Redis의 Single Thread 특성과 원자적 연산을 사용해 락을 사용하지 않고도 동시성 제어를 하여 데이터 무결성을 확보할 수 있었습니다.
    
    읽기 전략은 Look Aside 방식을 도입하여 클라이언트가 특정 데이터를 읽을 때마다 캐시를 먼저 확인하고, Cache miss의 경우 Database에서 Data를 가져와 캐시에 저장한 후 클라이언트에 반환합니다. 이 방식을 통해 Database와 캐시 간의 일관성을 유지할 수 있습니다.
    
    종합적으로 Redis를 통해 대규모 트랜잭션 상황에서의 동시성 제어를 하면서, 위의 캐시 전략으로 데이터의 정확성과 일관성을 보장해 데이터 무결성을 확보할 수 있었습니다.
    ```
    
- 📊 APM을 활용해서 시스템의 성능과 안정성을 지속적으로 관리
    
    ```java
    대규모 트랜잭션 상황에서 동시성 제어를 수행하며, 프로젝트의 챌린지 포인트 중 하나로 APM을 활용한 모니터링을 도입하였습니다. 
    이를 통해 시스템의 성능과 안정성을 지속적으로 관리하고 개선할 수 있었습니다. 
    
    저희 팀은 Grafana, Cloud Watch 및 Pinpoint와 같은 다양한 모니터링 도구를 사용하여 시스템의 전반적인 성능을 실시간으로 확인하였습니다.
     
    특히, Grafana와 Cloud Watch를 통해 EC2, Elastic Cache, ALB, RDS, Auto Scaling 등의 상태를 실시간으로 모니터링할 수 있었습니다. 
    
    또한, Pinpoint를 사용하여 병목 현상이 나타나는 지점을 확인하고 개선할 수 있었습니다. 이를 통해 서비스의 안정성과 성능을 지속적으로 유지하고 개선할 수 있었으며, 사용자들에게 최상의 서비스 경험을 제공할 수 있게 되었습니다. 
    
    종합적으로, 이러한 모니터링 도구들의 활용을 통해 시스템 내 문제가 발생했을 때 빠르게 진단하고 수정 및 개선 작업을 수행할 수 있었습니다. 결과적으로 프로젝트는 안정성과 높은 성능을 보장하는 성공적인 구현이 이루어졌습니다.  
    ```
    
- 📉 비용을 최소화하면서도 높은 가용성과 성능을 유지하는 가성비 최적화 전략
    
    ```java
    저희는 최소 비용으로도 높은 가용성과 성능을 유지하는 것을 목표로 하며, 이를 위해 다양한 기술적인 방법과 전략적인 설계를 채택하였습니다. 
    
    EC2는 직전 버전보다 20% 저렴한 Graviton2 arm64 아키텍쳐 기반에, 무료로도 사용 가능한 t4g.small 서버를 사용했습니다. 서버 확장이 필요할 경우 비용이 더 발생하는 Scale Up 방식보다 Load Balancing을 이용해 Scale Out 방식의 수평적 확장으로 비용을 최소화 하였습니다.
    
    공연 예매 사이트의 특성상 예매 오픈 시간대에 트래픽이 집중되는 현상이 발생합니다. 그래서 Auto Scaling을 설정해 오픈 시간 직전과 트랜잭션이 몰리는 상황에서만 서버 인스턴스 확장을 하고, 서버 부하가 없는 대부분의 시간에는 서버 인스턴스가 최소로 유지됩니다. 서버 수를 동적으로 조절하여 자원 사용량을 최적화하고 비용 절감 효과를 극대화할 수 있었습니다.
    
    이를 통해 높은 가용성과 성능을 유지하면서도 비용을 최소화하는 것뿐만 아니라, 가성비 측면에서도 최적의 결과를 얻을 수 있도록 했습니다.
    ```
    

---

# 📚04 | 기술 스택

## ⚙ Architecture 구성도

![neticket3.png](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/neticket3.png)

## ⚔ 프로젝트에서 사용한 기술

### *💻 Backend*

---

📚 **Tech Stack**

- Spring Boot
- Spring JPA
- Spring Security
- JAVA
- JWT
- Redis Cache
- QueryDSL
- Caffeine

🔩 **DB**

- MySQL (AWS RDS, db.t4g.micro)
- Redis (AWS ElastiCache****)****

🗜 **DevOps**

- AWS EC2 (t4g.small Ubuntu/Linux)
- AWS S3
- AWS Application Load Balancer
- AWS Auto Scaling
- AWS Code Delploy
- GitHub Actions
- Docker

⚖ **Test**

- Junit5
- Mockito
- Jmeter
- Postman

🖥 **Monitoring**

- AWS CloudWatch
- Grafana
- Pinpoint

---

## 🏹 기술적 의사 결정

[기술적 의사 결정 과정 보러가기](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/%E1%84%80%E1%85%B5%E1%84%89%E1%85%AE%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%8B%E1%85%B4%E1%84%89%E1%85%A1%20%E1%84%80%E1%85%A7%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A5%E1%86%BC%20%E1%84%87%E1%85%A9%E1%84%85%E1%85%A5%E1%84%80%E1%85%A1%E1%84%80%E1%85%B5%2003c5ff1ae90d4b469ccdd24d417f347b.md)

## 🗺 API 명세서

[API 명세서](https://www.notion.so/API-ec546f91e7b0472ea3ab3909ffc1b2ee) 

## 💾 ERD

![Untitled](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/Untitled.png)

---

# 👾05 | Trouble Shooting

### 🌟 예매 서비스 로직에서 데이터 무결성을 지키며 응답속도와 TPS 개선 🌟

<aside>
💡 챌린지 포인트로 설정한 목표를 달성하기 위해
예매 서비스 로직에서 ‘남은 좌석 수’의 데이터 무결성을 지켜야 하고,
많은 트랜잭션을 처리하여 클라이언트에게 빠른 응답 속도로 예매 결과를 돌려줘야 합니다.

</aside>

[1. 트랜잭션 충돌 문제를 비관적 락으로 해결](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/1%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%20%E1%84%8E%E1%85%AE%E1%86%BC%E1%84%83%E1%85%A9%E1%86%AF%20%E1%84%86%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%A6%E1%84%85%E1%85%B3%E1%86%AF%20%E1%84%87%E1%85%B5%E1%84%80%E1%85%AA%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%85%E1%85%A1%E1%86%A8%E1%84%8B%E1%85%B3%E1%84%85%E1%85%A9%20%E1%84%92%E1%85%A2%E1%84%80%20eafef23a99014be8b9ce17d0166cce66.md)

[2. 비관적 락 적용 시 속도 문제를 Redis Cache로 해결](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/2%20%E1%84%87%E1%85%B5%E1%84%80%E1%85%AA%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%85%E1%85%A1%E1%86%A8%20%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%89%E1%85%B5%20%E1%84%89%E1%85%A9%E1%86%A8%E1%84%83%E1%85%A9%20%E1%84%86%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%A6%E1%84%85%E1%85%B3%E1%86%AF%20Redis%20Cach%2021a65fa3a8a242868a2e60bdf785752d.md)

[3. 더 높은 목표치를 Scale Out으로 해결](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/3%20%E1%84%83%E1%85%A5%20%E1%84%82%E1%85%A9%E1%87%81%E1%84%8B%E1%85%B3%E1%86%AB%20%E1%84%86%E1%85%A9%E1%86%A8%E1%84%91%E1%85%AD%E1%84%8E%E1%85%B5%E1%84%85%E1%85%B3%E1%86%AF%20Scale%20Out%E1%84%8B%E1%85%B3%E1%84%85%E1%85%A9%20%E1%84%92%E1%85%A2%E1%84%80%E1%85%A7%E1%86%AF%20bec5d753bb48430eac90d9cd92ea8e6a.md)

[4. 서버 확장으로 인한 비용 문제를 AutoScaling으로 해결](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/4%20%E1%84%89%E1%85%A5%E1%84%87%E1%85%A5%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B3%E1%84%85%E1%85%A9%20%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%92%E1%85%A1%E1%86%AB%20%E1%84%87%E1%85%B5%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%86%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%A6%E1%84%85%E1%85%B3%E1%86%AF%20AutoScalin%20b956bdd657be4f47be26217c6ba87231.md)

[5. APM으로 찾은 병목현상을 Hikari, MySQL 튜닝으로 해결](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/5%20APM%E1%84%8B%E1%85%B3%E1%84%85%E1%85%A9%20%E1%84%8E%E1%85%A1%E1%86%BD%E1%84%8B%E1%85%B3%E1%86%AB%20%E1%84%87%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A9%E1%86%A8%E1%84%92%E1%85%A7%E1%86%AB%E1%84%89%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B3%E1%86%AF%20Hikari,%20MySQL%20%E1%84%90%E1%85%B2%E1%84%82%201ab511e3fdf24865aaf1b5d56d03e228.md)

---

# 🚀06 | 성능 개선

<aside>
🛠 저희는 트러블 슈팅의 각 단계에서 성능이 어느 정도까지 개선되었는지 알아보기 위해,
프로젝트 내내 Apache Jmeter를 통해 부하테스트를 진행했습니다.
평균 응답 속도(ms)와 처리량(TPS, Transacion Per Second)을 위주로 정리해 두었습니다.

</aside>

<aside>
⚠️ 비관적 락 : 트러블슈팅 1단계, 비관적 락으로 문제를 해결했을 당시 테스트 값
Redis : 트러블슈팅 2단계, 예매 로직에서 남은 좌석 수를 Redis로 해결했을 당시 테스트 값
Scale Out : 트러블슈팅 3단계, 수평적 확장으로 문제를 해결했을 당시 테스트 값
최종 : 트러블슈팅 5단계 이후, 모든 개선이 끝난 뒤 테스트 값

</aside>

### 1. GET 상세 페이지 요청 테스트 결과

- 상세페이지 조회 API에 동시에 GET 읽기 요청을 보내는 테스트입니다.

![Untitled](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/Untitled%201.png)

<aside>
🔥 동시 요청 수(Jmeter 쓰레드) 10,000 테스트 결과 값입니다.

비관적 락 적용 당시에는 모든 요청을 DB로 처리했지만, Redis 적용 후 상세 페이지 조회 반환 DTO에 캐시를 적용해 응답속도를 크게 개선하였습니다.

비관적 락 적용 당시와 최종 개선의 테스트 결과 값들을 비교해보면,

평균 응답 속도는 22,848ms → 105ms로 약 **99.54%** 개선되었습니다.

처리량은 176 tps → 1,508 tps로 **7.59배** 증가했습니다.

</aside>

---

| 라벨 | 표본 수 | 평균
응답
속도 | 표준편차 | 오류 % | 처리량(tps) | 수신 KB/초 | 전송 KB/초 | 평균 바이트 수 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| GET  
상세 페이지 조회 | 50000 | 194 ms | 521.570752344983 | 0.0 | 1425.2323128669973 | 846.2316857647796 | 261.66374494042526 | 608.0 |

<aside>
🔥 동시 요청 수(Jmeter 쓰레드) 50,000일 때 최종 테스트의 결과 값입니다.

평균 응답 속도는 **194ms**, 처리량은 **1425 tps** 입니다.

</aside>

### 2. POST 예매하기 요청 테스트 결과

- 예매하기 API에 동시에 POST 쓰기 요청을 보내는 테스트입니다.
- 비관적 락 적용 이전 로직에서는 요청이 50개만 있어도 LockAcquisitionException이 발생해 이 테스트를 진행할 수 없어 제외했습니다.

![Untitled](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/Untitled%202.png)

<aside>
🔥 동시 요청 수(Jmeter 쓰레드) 10,000 테스트 결과 값입니다.

예매 로직에 Redis Cache 적용 후 크게 개선되었습니다.

비관적 락 적용 당시와 최종 개선의 테스트 결과 값들을 비교해보면,

평균 응답 속도는 49,368ms → 960ms로 약 **98.06%** 개선되었습니다.

처리량은 77 tps → 1,223 tps로 **15.87배** 증가했습니다.

</aside>

---

![Untitled](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/Untitled%203.png)

<aside>
🔥 동시 요청 수(Jmeter 쓰레드) 20,000 테스트 결과 값입니다.

비관적 락 적용 당시와 최종 개선의 테스트 결과 값들을 비교해보면,

평균 응답 속도는 36,239ms → 1312ms로 약 **96.37%** 개선되었습니다.

처리량은 154 tps → 1,258 tps로 **7.16배** 증가했습니다.

</aside>

---

| 라벨 | 표본 수 | 평균 응답 시간 | 표준편차 | 오류 % | 처리량(tps) | 수신 KB/초 | 전송 KB/초 | 평균 바이트 수 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Post 예매 | 50000 | 1338 ms | 1153.6917160859907 | 0.0 | 1097.9599903379521 | 359.1958952765761 | 498.58534717494894 | 335.0 |

<aside>
🔥 동시 요청 수(Jmeter 쓰레드) 50,000일 때 최종 테스트의 결과 값입니다.

평균 응답 속도는 **1338ms**, 처리량은 **1097 tps** 입니다.

</aside>

### 3. 시나리오 테스트 결과

- 실제 유저가 사이트를 이용한다는 가정하에 시나리오를 작성하여 테스트 한 결과 값입니다.
- 상세페이지 조회 GET 요청, 메인페이지 조회 GET 요청, 예매 중 페이지 조회 GET요청, 예매하기 POST 요청을 동시에 진행했습니다.

![Untitled](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/Untitled%204.png)

<aside>
🔥 동시 요청 수(Jmeter 쓰레드) 2,000 / 루프 카운트 무한 테스트 결과 값입니다.
매 초 2,000명 사용자가 4개의 API에 요청을 보내 초당 8,000 요청이 계속 들어갑니다.

비관적 락 적용 당시는 해당 테스트를 진행하지 못 했습니다.

Redis 적용 당시와 최종 개선의 테스트 결과 값들을 비교해보면,

평균 응답 속도는 2,542ms → 1,031ms로 약 **59.47%** 개선되었습니다.

처리량은 776 tps → 1,816 tps로 **2.34배** 증가했습니다.

</aside>

---

| 라벨 | 표본 수 | 평균 응답 속도 | 표준편차 | 오류 % | 처리량(tps) | 수신 KB/초 | 전송 KB/초 | 평균 바이트 수 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| GET / 상세페이지 조회 | 10000 | 2053 | 1640.9176094105364 | 0.0 | 440.43162299053074 | 268.81812926668135 | 80.86049328341775 | 625.0 |
| GET / 메인페이지 조회  | 10000 | 1545 | 1274.5435510707982 | 0.0 | 411.09969167523127 | 623.4896100976362 | 77.48265673175746 | 1553.0378 |
| GET / 예매 중 확인하기  | 10000 | 1526 | 1205.9461391911293 | 0.0 | 402.7548431269886 | 245.82204780699988 | 75.90984836280155 | 625.0 |
| POST / 예매하기  | 10000 | 1595 | 1264.8335487780832 | 0.0 | 402.77106492669566 | 131.76592456097956 | 182.89896991300145 | 335.0 |
| 총계 | 40000 | 1680 ms | 1374.744645600469 | 0.0 | 1594.7055774827572 | 1221.7398393583305 | 404.51638061635373 | 784.50945 |

<aside>
🔥 동시 요청 수(Jmeter 쓰레드) 10,000일 때 최종 시나리오 테스트의 결과 값입니다.
1초 동안 API당 10,000요청, 총 40,000 요청이 들어갔습니다.

평균 응답 속도는 **1680 ms**, 처리량은 **1594 tps** 입니다.

</aside>

---

# 👨‍👨‍👧‍👦07 | Team

[Team (1)](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/Team%20(1)%205001f4a99c1541da90ff266c7fc2d755.md)

![Untitled](%5BNETicket%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8C%E1%85%A2%E1%86%A8%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B3%E1%86%AF%20%E1%84%88%E1%85%A1%E1%84%85%E1%85%B3%E1%84%80%E1%85%A9%20%E1%84%8B%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%20a6352070cc0147a18ea9597094123290/Untitled%205.png)